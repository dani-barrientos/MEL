/**
\page execution_sample_advance Advanced example

Although the code shown in this section is not really complicated, it's a more real one and allow us to check performance aspects.
The goal of this example is simply to calculate the mean of a very big vector. We will have a template function to abstract from execution
agent. This is the code:
\code
template <class ExecutorType> void _testMeanVector(::mel::execution::ExFuture<ExecutorType,vector<double>> fut,tests::BaseTest* test)
{
	typedef vector<double> VectorType;
	try
	{
		Timer timer;
		uint64_t t0 = timer.getMilliseconds();

		auto res5 = mel::core::waitForFutureThread(	
		fut
		| mel::execution::parallel_convert<std::tuple<double,double,double,double>>(  //calculate mean in 4 parts @todo ¿cómo podrái devolver este resultado a siguiente funcion?
			[](VectorType& v) noexcept
			{
				double mean = 0.f;
				size_t tam = v.size()/4;
				size_t endIdx = tam;
				for(size_t i = 0; i < endIdx;++i)
					mean += v[i];
				mean /= v.size();
				return mean;
			},
			[](VectorType& v) noexcept
			{
				double mean = 0.f;
				size_t tam = v.size()/4;
				size_t startIdx = tam;
				size_t endIdx = tam*2;
				for(size_t i = startIdx; i < endIdx;++i)
					mean += v[i];
				mean /= v.size();
				return mean;
			},
			[](VectorType& v) noexcept
			{
				double mean = 0.f;
				size_t tam = v.size()/4;
				size_t startIdx = tam*2;
				size_t endIdx = tam*3;
				for(size_t i = startIdx; i < endIdx;++i)
					mean += v[i];
				mean /= v.size();
				return mean;
			},
			[](VectorType& v) noexcept
			{
				double mean = 0.f;
				size_t tam = v.size()/4;
				size_t startIdx = tam*3;
				size_t endIdx = v.size();
				for(size_t i = startIdx; i < endIdx;++i)
					mean += v[i];
				mean /= v.size();
				return mean;
			}
		) | mel::execution::next( [](std::tuple<double,double,double,double>& means)
		{
			return (std::get<0>(means)+std::get<1>(means)+std::get<2>(means)+std::get<3>(means));
		}));
		uint64_t t1 = timer.getMilliseconds();
		text::info("Mean = {}. Time spent = {} seconds",res5.value(),(float)((t1-t0)/1000.f));
	}catch(std::exception& e)
	{
		text::info("Error = {}",e.what());
	}
}
\endcode
And from the *main* function:
\code
    auto th1 = ThreadRunnable::create(true);
	execution::Executor<Runnable> exr(th1);
	typedef vector<double> VectorType;
	auto initFut = execution::launch(exr,[]()
	{				
		constexpr int vecSize = 10'000'000;
		VectorType v(vecSize);
		for( size_t i = 0; i < vecSize;++i)
			v[i] = (std::rand()%20)/3.0; //to create a double
		return v;
	});
	core::waitForFutureThread(initFut); //wait for completion of vector creation to not interfere in time measurement
	//plain version
	text::info("vector mean: plain way");
    Timer timer;
	uint64_t t0 = timer.getMilliseconds();
	auto mean = core::waitForFutureThread( initFut | 
	execution::next([](VectorType& v) noexcept
	{
		double mean = 0.0;
		size_t endIdx = v.size();
		for(size_t i = 0; i < endIdx;++i)
			mean+=v[i];
		mean/=v.size();
		return mean;
	}));
	uint64_t t1 = timer.getMilliseconds();
	mel::text::info("Mean = {}. Time spent = {}",mean.value(),(float)((t1-t0)/1000.f));	
	{		
		parallelism::ThreadPool::ThreadPoolOpts opts;
		opts.threadOpts.schedulerOpts = ProcessScheduler::LockFreeOptions{};
		auto myPool = make_shared<parallelism::ThreadPool>(opts);
		parallelism::ThreadPool::ExecutionOpts exopts;
		execution::Executor<parallelism::ThreadPool> extp(myPool);
		extp.setOpts({true,true});
		text::info("vector mean: ThreadPoolExecutor");
		_testMeanVector( execution::transfer(initFut,extp),test);
	}
	{
		exr.setOpts({true,false});
		text::info("vector mean: RunnableExecutor");
		
		_testMeanVector(initFut,test);
	}
\endcode
In this code, a very big vector is created, and the mean of its values is calculated by:
    - spliting the vector in 4 parts and calculating the mean on each. this is done in using \ref mel::execution::parallel_convert "parallel_convert" 
    with 4 *parallel* tasks, each of which returns its own mean, so the full result of this job is a tuple.
    - passing the previous tuple with \ref mel::execution::next "next" to a callable which sums the four measurements.

Also, at first, a plain calculation is done to compare de results. In the concrete machine uses at this moment, the output is:
\code
[2022-04-25 10:15:05.215] [info] vector mean: plain way
[2022-04-25 10:15:05.224] [info] Mean = 3.167751133310629. Time spent = 0.009
[2022-04-25 10:15:05.227] [info] vector mean: ThreadPoolExecutor
[2022-04-25 10:15:05.232] [info] Mean = 3.1677511333276387. Time spent = 0.004 seconds
[2022-04-25 10:15:05.232] [info] vector mean: RunnableExecutor
[2022-04-25 10:15:05.243] [info] Mean = 3.1677511333276387. Time spent = 0.01 seconds
\endcode
The results show two very good news:
    - using a \ref mel::execution::ThreadPoolExecutor "ThreadPoolExecutor" is more than double faster than a simple \ref ::mel::execution::RunnableExecutor "RunnableExecutor".
    By default a ThreadPool uses all available cores. In this used machine, the number of cores is 8, so each task in *parallel_convert* is fully independent. Althoguh apparently this 
    should mean that the code should be 4 times faster, other things can affect performance, mainly cache issues.
    - the direct way takes almost the same time that the *RunnableExecutor* way, meaning that the internals of the execution and microthread system is very,very low
*/
